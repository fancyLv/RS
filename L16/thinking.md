# Thinking
## 1.机器学习中的监督学习、非监督学习、强化学习有何区别
监督学习是机器学习任务的一种。它从有标记的训练数据中推导出预测函数。有标记的训练数据是指每个训练实例都包括输入和期望的输出。  
无监督学习是机器学习任务的一种。它从无标记的训练数据中推断结论。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。  
强化学习是机器学习的另一个领域。它关注的是软件代理如何在一个环境中采取行动以便最大化某种累积的回报。  
强化学习与监督学习的区别，没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的。  
与非监督学习的区别，在非监督学习中既没有输出值也没有奖励值的，只有数据特征，而强化学习有奖励值（为负是为惩罚），此外非监督学习与监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。  

## 2.什么是策略网络，价值网络，有何区别
策略网络就是，对于给定的输入，通过学习给出一个确定输出的网络。  
价值网络：通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络。奖励更多的状态，会在数值网络中的数值Value更大。这里的奖励是奖励期望值，我们会从状态集合中选择最优的。  
价值网络的输出，一个可能获胜的数值，即“价值”，这个价值训练是一种回归(regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测。

## 3.请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select,  Expansion，Simluation，Backpropagation是如何操作的
蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性。MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树。
MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域。  
使用主要步骤：  
选择，从根节点开始，按一定策略，搜索到叶子节点  
扩展，对叶子节点扩展一个或多个合法的子节点  
模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数  
回传，根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点  

## 4.假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑
用户可以通过翻页来实现与推荐系统的多轮交互，此过程中推荐系统能够感知用户的实时行为，从而更加理解用户，在接下来的交互中提供更好的体验。
在这样的多轮交互中，我们把推荐系统看作智能体（Agent），用户看作环境（Environment），推荐系统与用户的多轮交互过程可以建模为MDP：
State：Agent对Environment的观测，即用户的意图和所处场景。
Action：以List-Wise粒度对推荐列表做调整，考虑长期收益对当前决策的影响。
Reward：根据用户反馈给予Agent相应的奖励，为业务目标直接负责。
P(s,a)：Agent在当前State s下采取Action a的状态转移概率。

## 5.在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路
自动驾驶的人工智能包含了感知、决策和控制三个方面。
感知指的是如何通过摄像头和其他传感器的输入解析出周围环境的信息，例如有哪些障碍物、障碍物的速度和距离、道路的宽度和曲率等。
自动驾驶的决策是指给定感知模块解析出的环境信息如何控制汽车的行为达到驾驶的目标。例如，汽车加速、减速、左转、右转、换道、超车都是决策模块的输出。