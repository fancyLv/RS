{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/santander-product-recommendation/test_ver2.csv.zip\n",
      "/kaggle/input/santander-product-recommendation/sample_submission.csv.zip\n",
      "/kaggle/input/santander-product-recommendation/train_ver2.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/santander-product-recommendation/train_ver2.csv.zip')\n",
    "test = pd.read_csv('/kaggle/input/santander-product-recommendation/test_ver2.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "## 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['age'].astype('str').str.contains('NA'),'age'] = np.nan\n",
    "train['age'] = pd.to_numeric(train['age'], errors='coerce')\n",
    "train['antiguedad'] = pd.to_numeric(train['antiguedad'], errors='coerce')\n",
    "train['renta'] = pd.to_numeric(train['renta'], errors='coerce')\n",
    "train.loc[train['antiguedad']<0,'antiguedad'] = 0\n",
    "\n",
    "test['age'] = pd.to_numeric(test['age'], errors='coerce')\n",
    "test['antiguedad'] = pd.to_numeric(test['antiguedad'], errors='coerce')\n",
    "test['renta'] = pd.to_numeric(test['renta'], errors='coerce')\n",
    "test.loc[test['antiguedad']<0,'antiguedad'] = 0\n",
    "\n",
    "train['indrel_1mes'] = train['indrel_1mes'].map({1.0:'1', '1.0':'1', '1':'1', \n",
    "                           2.0:'2', '2.0':'2', '2':'2', \n",
    "                          3.0:'3', '3.0':'3','3':'3', \n",
    "                          4.0:'4','4.0':'4', '4':'4',\n",
    "                         'P':'P'})\n",
    "test['indrel_1mes'] = test['indrel_1mes'].map({1.0:'1', '1.0':'1', '1':'1', \n",
    "                           2.0:'2', '2.0':'2', '2':'2', \n",
    "                          3.0:'3', '3.0':'3','3':'3', \n",
    "                          4.0:'4','4.0':'4', '4':'4',\n",
    "                         'P':'P'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data = train.select_dtypes(include=[\"object\"])\n",
    "missing_columns = [col for col in string_data if string_data[col].isnull().any()]\n",
    "# for col in missing_columns:\n",
    "#     print(\"Unique values for {0}:\\n{1}\\n\".format(col,string_data[col].unique()))\n",
    "del string_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['indfall'] = train['indfall'].fillna('N')\n",
    "train['tiprel_1mes'] = train['tiprel_1mes'].fillna('A')\n",
    "train['indrel_1mes'] = train['indrel_1mes'].fillna('P')\n",
    "\n",
    "test['indfall'] = test['indfall'].fillna('N')\n",
    "test['tiprel_1mes'] = test['tiprel_1mes'].fillna('A')\n",
    "test['indrel_1mes'] = test['indrel_1mes'].fillna('P')\n",
    "\n",
    "unknown_cols = [col for col in missing_columns if col not in [\"indfall\",\"tiprel_1mes\",\"indrel_1mes\"]]\n",
    "\n",
    "for column in unknown_cols:\n",
    "    train[column] = train[column].fillna('UNKNOWN')\n",
    "    test[column] = test[column].fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类特征LabelEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['ind_empleado', 'pais_residencia', 'sexo', 'indrel_1mes', 'tiprel_1mes',\n",
    "               'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'segmento']\n",
    "\n",
    "encoders = []\n",
    "for i in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    train[i] = le.fit_transform(train[i])\n",
    "    test[i] = le.transform(test[i])\n",
    "    encoders.append(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ind_empleado', 'pais_residencia', 'sexo',\n",
    "            'age', 'ind_nuevo', 'antiguedad', 'indrel',\n",
    "             'indrel_1mes', 'tiprel_1mes', 'indresi', 'indext',\n",
    "            'conyuemp', 'canal_entrada', 'indfall', 'cod_prov',\n",
    "            'ind_actividad_cliente', 'renta', 'segmento']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train.iloc[:1,].filter(regex=\"ind_+.*ult.*\").columns.values\n",
    "targets = targets[2:]\n",
    "\n",
    "for column in targets:\n",
    "    train[column] = train[column].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用2015-06-28数据进行训练，2015-01-28，2015-05-28数据构造新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['fecha_dato']=='2015-01-28')|(train['fecha_dato']=='2015-05-28')|(train['fecha_dato']=='2015-06-28')]\n",
    "\n",
    "lag_cust = train[(train['fecha_dato']=='2015-01-28')][['ncodpers']+list(targets)]\n",
    "cust = train[(train['fecha_dato']=='2015-05-28')][['ncodpers']+list(targets)]\n",
    "lag_cust2 = train[(train['fecha_dato']=='2016-01-28')][['ncodpers']+list(targets)]\n",
    "cust2 = train[(train['fecha_dato']=='2016-05-28')][['ncodpers']+list(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[train['fecha_dato']=='2015-06-28'][['ncodpers']+features].merge(cust,on='ncodpers',how='left').merge(lag_cust,on='ncodpers',how='left')\n",
    "test_data = test[['ncodpers']+features].merge(cust2,on='ncodpers',how='left').merge(lag_cust2,on='ncodpers',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in targets:\n",
    "    train_data[i+'_x'] = train_data[i+'_x'].fillna(0)\n",
    "    train_data[i+'_y'] = train_data[i+'_y'].fillna(0)\n",
    "    test_data[i+'_x'] = test_data[i+'_x'].fillna(0)\n",
    "    test_data[i+'_y'] = test_data[i+'_y'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = train[train['fecha_dato'] == '2015-06-28'][['ncodpers']+list(targets)].merge(train[train['fecha_dato'] == '2015-05-28'][['ncodpers']+list(targets)],\n",
    "                                                                                         on='ncodpers', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df_1 = target_df[['ncodpers']+[i+'_x' for i in targets]]\n",
    "target_df_1.columns = ['ncodpers'] + list(targets)\n",
    "target_df_2 = target_df[['ncodpers']+[i+'_y' for i in targets]]\n",
    "target_df_2.columns = ['ncodpers'] + list(targets)\n",
    "target_df_1.set_index('ncodpers',inplace=True)\n",
    "target_df_2.set_index('ncodpers',inplace=True)\n",
    "\n",
    "target_df = (target_df_1 - target_df_2).reset_index()\n",
    "target_df[target_df<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_cco_fin_ult1 6588.0\n",
      "ind_cder_fin_ult1 9.0\n",
      "ind_cno_fin_ult1 1857.0\n",
      "ind_ctju_fin_ult1 7.0\n",
      "ind_ctma_fin_ult1 210.0\n",
      "ind_ctop_fin_ult1 213.0\n",
      "ind_ctpp_fin_ult1 151.0\n",
      "ind_deco_fin_ult1 289.0\n",
      "ind_deme_fin_ult1 33.0\n",
      "ind_dela_fin_ult1 939.0\n",
      "ind_ecue_fin_ult1 1204.0\n",
      "ind_fond_fin_ult1 245.0\n",
      "ind_hip_fin_ult1 4.0\n",
      "ind_plan_fin_ult1 19.0\n",
      "ind_pres_fin_ult1 8.0\n",
      "ind_reca_fin_ult1 2931.0\n",
      "ind_tjcr_fin_ult1 4655.0\n",
      "ind_valo_fin_ult1 152.0\n",
      "ind_viv_fin_ult1 3.0\n",
      "ind_nomina_ult1 5070.0\n",
      "ind_nom_pens_ult1 8135.0\n",
      "ind_recibo_ult1 9023.0\n"
     ]
    }
   ],
   "source": [
    "for i in targets:\n",
    "    print(i,target_df[i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_data.columns\n",
    "train_data = train_data.merge(target_df,on='ncodpers')\n",
    "\n",
    "data_list = []\n",
    "for i,column in enumerate(targets):\n",
    "    data = train_data[train_data[column]>0][columns]\n",
    "    data['target'] = i\n",
    "    data_list.append(data)\n",
    "train_data = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data.drop(columns=['ncodpers','target']), train_data['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'boosting_type':'gbdt',\n",
    "         'objective' : 'multiclass',\n",
    "         'num_class':22,\n",
    "         'metric' : 'multi_logloss',\n",
    "         'learning_rate' : 0.01,\n",
    "         'max_depth' : 15,\n",
    "         'feature_fraction':0.8,\n",
    "         'bagging_fraction': 0.9,\n",
    "         'bagging_freq': 8,\n",
    "         'lambda_l1': 0.6,\n",
    "         'lambda_l2': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['canal_entrada', 'conyuemp', 'ind_empleado', 'indext', 'indfall', 'indrel_1mes', 'indresi', 'pais_residencia', 'segmento', 'sexo', 'tiprel_1mes']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[25]\ttraining's multi_logloss: 1.83789\tvalid_1's multi_logloss: 1.85691\n",
      "[50]\ttraining's multi_logloss: 1.66053\tvalid_1's multi_logloss: 1.69021\n",
      "[75]\ttraining's multi_logloss: 1.53281\tvalid_1's multi_logloss: 1.57058\n",
      "[100]\ttraining's multi_logloss: 1.43802\tvalid_1's multi_logloss: 1.48248\n",
      "[125]\ttraining's multi_logloss: 1.36348\tvalid_1's multi_logloss: 1.41441\n",
      "[150]\ttraining's multi_logloss: 1.30284\tvalid_1's multi_logloss: 1.36008\n",
      "[175]\ttraining's multi_logloss: 1.25432\tvalid_1's multi_logloss: 1.31784\n",
      "[200]\ttraining's multi_logloss: 1.21389\tvalid_1's multi_logloss: 1.28321\n",
      "[225]\ttraining's multi_logloss: 1.17998\tvalid_1's multi_logloss: 1.25527\n",
      "[250]\ttraining's multi_logloss: 1.15083\tvalid_1's multi_logloss: 1.23191\n",
      "[275]\ttraining's multi_logloss: 1.12539\tvalid_1's multi_logloss: 1.21217\n",
      "[300]\ttraining's multi_logloss: 1.10392\tvalid_1's multi_logloss: 1.19648\n",
      "[325]\ttraining's multi_logloss: 1.08518\tvalid_1's multi_logloss: 1.18345\n",
      "[350]\ttraining's multi_logloss: 1.06837\tvalid_1's multi_logloss: 1.17231\n",
      "[375]\ttraining's multi_logloss: 1.05369\tvalid_1's multi_logloss: 1.16317\n",
      "[400]\ttraining's multi_logloss: 1.04021\tvalid_1's multi_logloss: 1.15546\n",
      "[425]\ttraining's multi_logloss: 1.02819\tvalid_1's multi_logloss: 1.14906\n",
      "[450]\ttraining's multi_logloss: 1.01701\tvalid_1's multi_logloss: 1.14354\n",
      "[475]\ttraining's multi_logloss: 1.00696\tvalid_1's multi_logloss: 1.1389\n",
      "[500]\ttraining's multi_logloss: 0.997644\tvalid_1's multi_logloss: 1.13498\n",
      "[525]\ttraining's multi_logloss: 0.988789\tvalid_1's multi_logloss: 1.13156\n",
      "[550]\ttraining's multi_logloss: 0.980453\tvalid_1's multi_logloss: 1.12857\n",
      "[575]\ttraining's multi_logloss: 0.972832\tvalid_1's multi_logloss: 1.12623\n",
      "[600]\ttraining's multi_logloss: 0.965677\tvalid_1's multi_logloss: 1.12445\n",
      "[625]\ttraining's multi_logloss: 0.958837\tvalid_1's multi_logloss: 1.12277\n",
      "[650]\ttraining's multi_logloss: 0.952384\tvalid_1's multi_logloss: 1.12139\n",
      "[675]\ttraining's multi_logloss: 0.946289\tvalid_1's multi_logloss: 1.1204\n",
      "[700]\ttraining's multi_logloss: 0.940331\tvalid_1's multi_logloss: 1.11953\n",
      "[725]\ttraining's multi_logloss: 0.934727\tvalid_1's multi_logloss: 1.11867\n",
      "[750]\ttraining's multi_logloss: 0.92943\tvalid_1's multi_logloss: 1.11819\n",
      "[775]\ttraining's multi_logloss: 0.924324\tvalid_1's multi_logloss: 1.11781\n",
      "[800]\ttraining's multi_logloss: 0.91938\tvalid_1's multi_logloss: 1.11762\n",
      "[825]\ttraining's multi_logloss: 0.914568\tvalid_1's multi_logloss: 1.11745\n",
      "[850]\ttraining's multi_logloss: 0.910011\tvalid_1's multi_logloss: 1.11743\n",
      "[875]\ttraining's multi_logloss: 0.90559\tvalid_1's multi_logloss: 1.11772\n",
      "[900]\ttraining's multi_logloss: 0.901192\tvalid_1's multi_logloss: 1.11795\n",
      "[925]\ttraining's multi_logloss: 0.896925\tvalid_1's multi_logloss: 1.11827\n",
      "[950]\ttraining's multi_logloss: 0.892799\tvalid_1's multi_logloss: 1.11863\n",
      "[975]\ttraining's multi_logloss: 0.888856\tvalid_1's multi_logloss: 1.11898\n",
      "[1000]\ttraining's multi_logloss: 0.884999\tvalid_1's multi_logloss: 1.11942\n",
      "[1025]\ttraining's multi_logloss: 0.881212\tvalid_1's multi_logloss: 1.11988\n",
      "Early stopping, best iteration is:\n",
      "[846]\ttraining's multi_logloss: 0.910732\tvalid_1's multi_logloss: 1.1174\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(param,train_data,valid_sets=[train_data,valid_data],num_boost_round = 10000 ,early_stopping_rounds=200,verbose_eval=25, categorical_feature=cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(test_data.drop(columns=['ncodpers']))\n",
    "preds_idx = np.argsort(-preds, axis=1)[:,:7]\n",
    "\n",
    "test_id = test['ncodpers']\n",
    "products = [' '.join(targets[i]) for i in preds_idx]\n",
    "\n",
    "df = pd.DataFrame({'ncodpers':test_id, 'added_products':products})\n",
    "df.to_csv('lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
