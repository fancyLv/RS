# Thinking

# 1 什么是近似最近邻查找，常用的方法有哪些
面对庞大的数据量以及数据库中高维的数据信息，现有的基于 NN 的检索方法无法获得理想的检索效果与可接受的检索时间。近似最近邻检索，搜索可能是近邻的数据项而不再只局限于返回最可能的项目，在牺牲可接受范围内的精度的情况下提高检索效率。
常用方法
树方法，如 KD-tree
哈希方法，如 Local Sensitive Hashing (LSH)
矢量量化方法，如 Product Quantization (PQ)
近邻图方法，如 Hierarchical Navigable Small World (HNSW)

# 2 为什么两个集合的minhash值相同的概率等于这两个集合的Jaccard相似度
考虑集合S1和S2，这两列所在的行有3种类型：  
1. S1和S2的值都为1，记为X  
2. 只有一个值为1，另一个值为0，记为Y  
3. S1和S2的值都为0，记为Z  
S1和S2交集的元素个数为x，并集的元素个数为x+y，所以sim(S1,S2) = Jaccard(S1,S2) = x/(x+y)。接下来计算h(S1)=h(S2)的概率，经过随机行打乱后，从上往下扫描，在碰到Y行之前碰到X行的概率为x/(x+y)，即h(S1)=h(S2)的概率为x/(x+y)。

# 3 SimHash在计算文档相似度的作用是怎样的？
SimHash算法：

1. 对文本分词，得到N维特征向量（默认为64维）
1. 为分词设置权重
1. 为特征向量计算哈希
1. 对所有特征向量加权，累加
1. 对累加结果，大于零置一，小于零置零
1. 得到文本指纹（fingerprint）

经过SimHash算法提取来的指纹（Simhash对长文本比较适用，短文本可能偏差较大，具体需要根据实际场景测试），最后使用海明距离，求相似，在google的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值，针对自己的应用可能又不同的测试取值

1. 将64位simhash均分为4份
1. 采用精确匹配的方式查找前16位
1. 找到则拿出来计算与被比较的simahsh距离，小于3则判断为相似
1. 如果样本库中存有2^34（差不多10亿）的哈希指纹，则每个table返回2^(34-16)=262144个候选结果，大大减少了海明距离的计算成本



# 4 为什么YouTube采用期望观看时间作为评估指标
CTR指标对于视频搜索具有一定的欺骗性，有些靠关键词吸引用户高点击的视频未必能够被播放，所以作者提出采用期望观看时间作为评估指标。
因为点击率容易提升欺骗性视频的排序，如标题党等。用户观看时间越长，收益越高，相比较点击率的计算更加准确。

# 5 为什么YouTube在排序阶段没有采用经典的LR（逻辑回归）当作输出层，而是采用了Weighted Logistic Regression
由于CTR指标对于视频搜索具有一定的欺骗性，所以论文采用期望观看时间作为评估指标，观看时长不是只有0，1两种标签，所以采用了Weighted Logistic Regression来模拟这个输出。在划分样本空间时，正样本为有点击视频，权重为观看时长；负样本为无点击视频，权重统一采用1，使用交叉熵损失函数进行训练，完成后，在serving阶段，最后DNN网络将输出,其即为odd,可以近似认为是期望的观看时长，以此做为每个视频的分数,并据此排序。

# Action2 
请设计一个基于DNN模型的推荐系统
阐述两阶段的架构（召回、排序）
以及每个阶段的DNN模型设计：

**DNN输入层（如何进行特征选择）**  
召回：  
embedded
video watches =>
watch vector，用户的历史观看是一个稀疏的，变长****的视频id序列，采用类似于word2vec的做法，每个视频都会被embedding到固定维度的向量中。最终通过加权平均（可根据重要性和时间进行加权）得到固定维度的watch
vector
embedded
search tokens => Search
vector，和watch
vector生成方式类似
用户画像特征：如地理位置，设备，性别，年龄，登录状态等连续或离散特征都被归一化为[0,1]，
和watch vector以及search vector做拼接（concatenate）

排序：  
相比召回阶段，引入了更多的feature（当前要计算的video的embedding，用户观看过的最后N个视频embedding的average，用户语言的embedding和当前视频语言的embedding，自上次观看同channel视频的时间，该视频已经被曝光给该用户的次数）

**DNN隐藏层结构**  
召回和排序采用Tower塔式模型，即第一层1024，第二层512，第三层256

**DNN输出层**  
召回：  
Training 阶段输出层为softmax层，
Serving
阶段直接用user Embedding和video Embedding计算dot-product表示分数，取topk作为候选结果。最重要问题是在性能。因此使用类似局部敏感哈希LSH（近似最近邻方法）  
排序：  
Ranking阶段的模型和召回阶段的基本相似，不同在于Training最后一层是Weighted
LR，Serving时激励函数使用的$e^{Wx+b}$







